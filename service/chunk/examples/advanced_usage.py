#!/usr/bin/env python3
"""
Chunk åˆ†å‰²å™¨é€²éšä½¿ç”¨ç¯„ä¾‹

é€™å€‹ç¯„ä¾‹å±•ç¤ºé€²éšåŠŸèƒ½ï¼ŒåŒ…æ‹¬è‡ªå®šç¾©åƒæ•¸ã€æ‰¹é‡è™•ç†ç­‰ã€‚
"""

import sys
from pathlib import Path
import logging
from typing import List

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from service.chunk import ChunkSplitter
from service.markdown_integrate import UnifiedMarkdownConverter
from langchain_core.documents import Document

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

def process_multiple_files():
    """æ‰¹é‡è™•ç†å¤šå€‹æ–‡ä»¶"""
    print("=== æ‰¹é‡è™•ç†å¤šå€‹æ–‡ä»¶ ===\n")
    
    # æ–‡ä»¶åˆ—è¡¨
    files = [
        "raw_docs/old_version/å°ç£äººå£½ç¾å¹´æœ‰é‘«ç¾å…ƒåˆ©ç‡è®Šå‹•å‹é‚„æœ¬çµ‚èº«ä¿éšª.pdf",
        # å¯ä»¥æ·»åŠ æ›´å¤šæ–‡ä»¶
    ]
    
    converter = UnifiedMarkdownConverter()
    all_chunks = []
    
    for file_path in files:
        if not Path(file_path).exists():
            print(f"âš ï¸ è·³éä¸å­˜åœ¨çš„æ–‡ä»¶: {file_path}")
            continue
        
        print(f"ğŸ”„ è™•ç†æ–‡ä»¶: {file_path}")
        
        # è½‰æ›æ–‡ä»¶
        result = converter.convert_file(file_path)
        
        # åˆ†å‰²å…§å®¹
        splitter = ChunkSplitter(
            chunk_size=800,           # è¼ƒå°çš„ chunk å¤§å°
            chunk_overlap=150,        # è¼ƒå°çš„é‡ç–Š
            normalize_output=True,
            keep_tables_together=True
        )
        
        chunks = splitter.split_markdown(input_data=result)
        all_chunks.extend(chunks)
        
        print(f"   âœ… å®Œæˆ: {len(chunks)} å€‹ chunks")
    
    print(f"\nğŸ“Š æ‰¹é‡è™•ç†çµæœ:")
    print(f"   - ç¸½æ–‡ä»¶æ•¸: {len(files)}")
    print(f"   - ç¸½ chunks: {len(all_chunks)}")
    
    return all_chunks

def custom_chunk_analysis(chunks: List[Document]):
    """è‡ªå®šç¾© chunk åˆ†æ"""
    print("\n=== è‡ªå®šç¾© Chunk åˆ†æ ===\n")
    
    # æŒ‰é ç¢¼åˆ†çµ„
    page_groups = {}
    for chunk in chunks:
        page_num = chunk.metadata.get('page_number')
        if page_num not in page_groups:
            page_groups[page_num] = []
        page_groups[page_num].append(chunk)
    
    print("ğŸ“„ æŒ‰é ç¢¼åˆ†çµ„çµ±è¨ˆ:")
    for page_num in sorted(page_groups.keys()):
        page_chunks = page_groups[page_num]
        total_length = sum(len(chunk.page_content) for chunk in page_chunks)
        avg_length = total_length / len(page_chunks)
        
        print(f"   é é¢ {page_num}: {len(page_chunks)} å€‹ chunks, å¹³å‡é•·åº¦: {avg_length:.1f}")
    
    # æŒ‰æ¨™é¡Œç´šæ•¸åˆ†çµ„
    header_groups = {}
    for chunk in chunks:
        header_level = get_header_level(chunk.metadata)
        if header_level not in header_groups:
            header_groups[header_level] = []
        header_groups[header_level].append(chunk)
    
    print("\nğŸ“‹ æŒ‰æ¨™é¡Œç´šæ•¸åˆ†çµ„çµ±è¨ˆ:")
    for level in sorted(header_groups.keys()):
        chunks_count = len(header_groups[level])
        print(f"   {level}ç´šæ¨™é¡Œ: {chunks_count} å€‹ chunks")
    
    # é•·åº¦åˆ†å¸ƒåˆ†æ
    lengths = [len(chunk.page_content) for chunk in chunks]
    lengths.sort()
    
    print(f"\nğŸ“ é•·åº¦åˆ†å¸ƒåˆ†æ:")
    print(f"   æœ€çŸ­: {min(lengths)} å­—ç¬¦")
    print(f"   æœ€é•·: {max(lengths)} å­—ç¬¦")
    print(f"   ä¸­ä½æ•¸: {lengths[len(lengths)//2]} å­—ç¬¦")
    
    # æ‰¾å‡ºç•°å¸¸é•·çš„ chunks
    long_chunks = [chunk for chunk in chunks if len(chunk.page_content) > 2000]
    if long_chunks:
        print(f"\nâš ï¸ ç•°å¸¸é•·çš„ chunks ({len(long_chunks)} å€‹):")
        for i, chunk in enumerate(long_chunks[:3], 1):
            print(f"   {i}. é•·åº¦: {len(chunk.page_content)}, é ç¢¼: {chunk.metadata.get('page_number')}")

def get_header_level(metadata):
    """ç²å–æ¨™é¡Œç´šæ•¸"""
    if 'Header 4' in metadata and metadata['Header 4']:
        return 'å››'
    elif 'Header 3' in metadata and metadata['Header 3']:
        return 'ä¸‰'
    elif 'Header 2' in metadata and metadata['Header 2']:
        return 'äºŒ'
    elif 'Header 1' in metadata and metadata['Header 1']:
        return 'ä¸€'
    else:
        return 'ç„¡'

def export_custom_analysis(chunks: List[Document]):
    """å°å‡ºè‡ªå®šç¾©åˆ†æçµæœ"""
    print("\n=== å°å‡ºè‡ªå®šç¾©åˆ†æ ===\n")
    
    # å‰µå»ºè‡ªå®šç¾©åˆ†å‰²å™¨ç”¨æ–¼å°å‡º
    splitter = ChunkSplitter()
    
    # å°å‡ºåˆ° Excel
    output_path = "service/output/chunk/advanced_analysis.xlsx"
    splitter._export_to_excel_with_page_info(
        chunks, 
        [],  # ç©ºçš„é é¢ä¿¡æ¯ï¼Œä½¿ç”¨å‚™ç”¨é‚è¼¯
        output_path
    )
    
    print(f"ğŸ“Š åˆ†æçµæœå·²å°å‡ºåˆ°: {output_path}")

def main():
    """é€²éšä½¿ç”¨ç¯„ä¾‹ä¸»å‡½æ•¸"""
    print("=== Chunk åˆ†å‰²å™¨é€²éšä½¿ç”¨ç¯„ä¾‹ ===\n")
    
    # 1. æ‰¹é‡è™•ç†
    all_chunks = process_multiple_files()
    
    if not all_chunks:
        print("âŒ æ²’æœ‰è™•ç†åˆ°ä»»ä½•æ–‡ä»¶")
        return
    
    # 2. è‡ªå®šç¾©åˆ†æ
    custom_chunk_analysis(all_chunks)
    
    # 3. å°å‡ºåˆ†æçµæœ
    export_custom_analysis(all_chunks)
    
    print(f"\nğŸ‰ é€²éšç¯„ä¾‹å®Œæˆï¼")

if __name__ == "__main__":
    main()
