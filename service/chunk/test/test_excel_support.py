#!/usr/bin/env python3
"""
æ¸¬è©¦ Excel æ–‡ä»¶æ”¯æ´åŠŸèƒ½

æ¸¬è©¦ ChunkSplitter å°æ²’æœ‰é é¢çµæ§‹çš„æ–‡ä»¶çš„è™•ç†èƒ½åŠ›ã€‚
"""

import sys
from pathlib import Path
import logging

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from service.chunk import ChunkSplitter
from service.markdown_integrate import UnifiedMarkdownConverter

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

def test_excel_file_conversion():
    """æ¸¬è©¦ Excel æ–‡ä»¶è½‰æ›"""
    print("=== æ¸¬è©¦ Excel æ–‡ä»¶è½‰æ›åŠŸèƒ½ ===\n")
    
    # 1. æº–å‚™è¼¸å…¥æ–‡ä»¶
    excel_file = "raw_docs/ç†è³ å¯©æ ¸åŸå‰‡.xlsx"
    
    if not Path(excel_file).exists():
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {excel_file}")
        print("è«‹ç¢ºä¿æ–‡ä»¶å­˜åœ¨æ–¼æŒ‡å®šè·¯å¾‘")
        return False
    
    print(f"ğŸ“„ è¼¸å…¥æ–‡ä»¶: {excel_file}")
    
    # 2. è½‰æ› Excel åˆ° Markdown
    print("\nğŸ”„ æ­¥é©Ÿ 1: è½‰æ› Excel åˆ° Markdown")
    converter = UnifiedMarkdownConverter()
    result = converter.convert_file(excel_file)
    
    print(f"âœ… è½‰æ›å®Œæˆ")
    print(f"   - æª”å: {result.metadata.file_name}")
    print(f"   - æª”æ¡ˆé¡å‹: {result.metadata.file_type}")
    print(f"   - è½‰æ›å™¨: {result.metadata.converter_used}")
    print(f"   - ç¸½é æ•¸: {result.metadata.total_pages}")
    print(f"   - ç¸½è¡¨æ ¼æ•¸: {result.metadata.total_tables}")
    print(f"   - é é¢ä¿¡æ¯: {'æœ‰' if result.pages and len(result.pages) > 0 else 'ç„¡'}")
    
    # 3. åˆ†å‰² Markdown å…§å®¹
    print("\nâœ‚ï¸ æ­¥é©Ÿ 2: åˆ†å‰² Markdown å…§å®¹")
    
    # å‰µå»ºåˆ†å‰²å™¨
    splitter = ChunkSplitter(
        chunk_size=800,           # è¼ƒå°çš„ chunk å¤§å°
        chunk_overlap=150,        # è¼ƒå°çš„é‡ç–Š
        normalize_output=True,    # å•Ÿç”¨å…§å®¹æ­£è¦åŒ–
        keep_tables_together=True # ä¿æŒè¡¨æ ¼å®Œæ•´æ€§
    )
    
    # åˆ†å‰²å…§å®¹
    chunks = splitter.split_markdown(
        input_data=result,
        output_excel=True,         # è¼¸å‡º Excel æ–‡ä»¶
        output_path="service/output/chunk/excel_test.xlsx",
        md_output_path="service/output/md/excel_test.md"
    )
    
    print(f"âœ… åˆ†å‰²å®Œæˆ: å…± {len(chunks)} å€‹ chunks")
    
    # 4. åˆ†æçµæœ
    print("\nğŸ“Š æ­¥é©Ÿ 3: åˆ†æçµæœ")
    
    # åŸºæœ¬çµ±è¨ˆ
    total_length = sum(len(chunk.page_content) for chunk in chunks)
    avg_length = total_length / len(chunks) if chunks else 0
    
    # æª¢æŸ¥é ç¢¼æƒ…æ³
    page_numbers = [chunk.metadata.get('page_number') for chunk in chunks]
    has_page_numbers = any(pn is not None for pn in page_numbers)
    
    # è¡¨æ ¼ chunks
    table_chunks = [chunk for chunk in chunks if chunk.metadata.get('is_table', False)]
    
    print(f"   - ç¸½ chunks: {len(chunks)}")
    print(f"   - ç¸½å­—ç¬¦æ•¸: {total_length}")
    print(f"   - å¹³å‡é•·åº¦: {avg_length:.1f}")
    print(f"   - è¡¨æ ¼ chunks: {len(table_chunks)}")
    print(f"   - ä¸€èˆ¬ chunks: {len(chunks) - len(table_chunks)}")
    print(f"   - é ç¢¼æƒ…æ³: {'æœ‰é ç¢¼' if has_page_numbers else 'ç„¡é ç¢¼ï¼ˆç¬¦åˆé æœŸï¼‰'}")
    
    # 5. æª¢æŸ¥ metadata
    print("\nğŸ“ æ­¥é©Ÿ 4: æª¢æŸ¥ Metadata")
    if chunks:
        sample_chunk = chunks[0]
        print(f"ç¯„ä¾‹ chunk metadata:")
        for key, value in sample_chunk.metadata.items():
            if key not in ['Header 1', 'Header 2', 'Header 3', 'Header 4']:
                print(f"   - {key}: {value}")
    
    # 6. é¡¯ç¤ºå‰å¹¾å€‹ chunks
    print("\nğŸ“„ æ­¥é©Ÿ 5: å‰ 3 å€‹ Chunks é è¦½")
    for i, chunk in enumerate(chunks[:3], 1):
        print(f"\n--- Chunk {i} ---")
        print(f"é•·åº¦: {len(chunk.page_content)}")
        print(f"é ç¢¼: {chunk.metadata.get('page_number', 'N/A')}")
        print(f"æ¨™é¡Œç´šæ•¸: {chunk.metadata.get('Header 1', 'N/A')}")
        print(f"å…§å®¹é è¦½: {chunk.page_content[:100]}...")
    
    print(f"\nğŸ‰ Excel æ–‡ä»¶è½‰æ›æ¸¬è©¦å®Œæˆï¼")
    print(f"ğŸ“ Excel æ–‡ä»¶: service/output/chunk/excel_test.xlsx")
    print(f"ğŸ“ Markdown æ–‡ä»¶: service/output/md/excel_test.md")
    
    return True

def test_metadata_consistency():
    """æ¸¬è©¦ metadata ä¸€è‡´æ€§"""
    print("\n=== æ¸¬è©¦ Metadata ä¸€è‡´æ€§ ===\n")
    
    excel_file = "raw_docs/ç†è³ å¯©æ ¸åŸå‰‡.xlsx"
    if not Path(excel_file).exists():
        print("âŒ æ¸¬è©¦æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³é metadata æ¸¬è©¦")
        return
    
    # è½‰æ›å’Œåˆ†å‰²
    converter = UnifiedMarkdownConverter()
    result = converter.convert_file(excel_file)
    
    splitter = ChunkSplitter(normalize_output=True)
    chunks = splitter.split_markdown(input_data=result)
    
    # æª¢æŸ¥æ‰€æœ‰ chunks çš„ metadata ä¸€è‡´æ€§
    required_fields = ['file_name', 'file_type', 'source', 'converter_used', 'total_pages', 'total_tables']
    
    print("æª¢æŸ¥å¿…è¦ metadata æ¬„ä½:")
    for field in required_fields:
        missing_count = sum(1 for chunk in chunks if field not in chunk.metadata)
        print(f"   - {field}: {len(chunks) - missing_count}/{len(chunks)} chunks åŒ…å«æ­¤æ¬„ä½")
    
    # æª¢æŸ¥é ç¢¼æƒ…æ³
    page_number_count = sum(1 for chunk in chunks if chunk.metadata.get('page_number') is not None)
    print(f"   - page_number: {page_number_count}/{len(chunks)} chunks æœ‰é ç¢¼ï¼ˆé æœŸç‚º 0ï¼‰")
    
    print("âœ… Metadata ä¸€è‡´æ€§æª¢æŸ¥å®Œæˆ")

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("=== Excel æ–‡ä»¶æ”¯æ´åŠŸèƒ½æ¸¬è©¦ ===\n")
    
    try:
        # æ¸¬è©¦åŸºæœ¬è½‰æ›åŠŸèƒ½
        success = test_excel_file_conversion()
        
        if success:
            # æ¸¬è©¦ metadata ä¸€è‡´æ€§
            test_metadata_consistency()
            
            print(f"\nğŸ‰ æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼")
        else:
            print(f"\nâŒ æ¸¬è©¦å¤±æ•—")
            
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
