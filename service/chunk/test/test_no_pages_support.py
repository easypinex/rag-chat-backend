#!/usr/bin/env python3
"""
æ¸¬è©¦æ²’æœ‰é é¢çµæ§‹çš„æ–‡ä»¶æ”¯æ´åŠŸèƒ½

æ¨¡æ“¬æ²’æœ‰é é¢ä¿¡æ¯çš„ ConversionResult ä¾†æ¸¬è©¦ _split_without_pages æ–¹æ³•ã€‚
"""

import sys
from pathlib import Path
import logging
from typing import List

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from service.chunk import ChunkSplitter
from service.markdown_integrate.data_models import ConversionResult, ConversionMetadata, PageInfo
from langchain_core.documents import Document

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

def create_mock_conversion_result_without_pages():
    """å‰µå»ºä¸€å€‹æ²’æœ‰é é¢ä¿¡æ¯çš„æ¨¡æ“¬ ConversionResult"""
    
    # æ¨¡æ“¬çš„ Markdown å…§å®¹
    mock_content = """# æ¸¬è©¦æ–‡æª”

é€™æ˜¯ä¸€å€‹æ¸¬è©¦æ–‡æª”ï¼Œç”¨æ–¼é©—è­‰æ²’æœ‰é é¢çµæ§‹çš„æ–‡ä»¶è™•ç†ã€‚

## ç¬¬ä¸€ç¯€

é€™æ˜¯ç¬¬ä¸€ç¯€çš„å…§å®¹ï¼ŒåŒ…å«ä¸€äº›åŸºæœ¬ä¿¡æ¯ã€‚

### å­æ¨™é¡Œ 1.1

é€™æ˜¯å­æ¨™é¡Œçš„å…§å®¹ã€‚

## ç¬¬äºŒç¯€

é€™æ˜¯ç¬¬äºŒç¯€çš„å…§å®¹ã€‚

### å­æ¨™é¡Œ 2.1

é€™æ˜¯å¦ä¸€å€‹å­æ¨™é¡Œçš„å…§å®¹ã€‚

## è¡¨æ ¼ç¯„ä¾‹

| æ¬„ä½1 | æ¬„ä½2 | æ¬„ä½3 |
|-------|-------|-------|
| å€¼1   | å€¼2   | å€¼3   |
| å€¼4   | å€¼5   | å€¼6   |

## çµè«–

é€™æ˜¯æ–‡æª”çš„çµè«–éƒ¨åˆ†ã€‚
"""
    
    # å‰µå»º metadata
    metadata = ConversionMetadata(
        file_name="test_no_pages.md",
        file_path="test_no_pages.md",
        file_type=".md",
        file_size=len(mock_content),
        total_pages=0,  # æ²’æœ‰é é¢
        total_tables=1,
        total_content_length=len(mock_content),
        conversion_timestamp=1234567890.0,
        converter_used="test_converter"
    )
    
    # å‰µå»º ConversionResultï¼Œä¸è¨­ç½® pagesï¼ˆæˆ–è¨­ç½®ç‚ºç©ºåˆ—è¡¨ï¼‰
    result = ConversionResult(
        content=mock_content,
        metadata=metadata,
        pages=None  # æ²’æœ‰é é¢ä¿¡æ¯
    )
    
    return result

def test_no_pages_splitting():
    """æ¸¬è©¦æ²’æœ‰é é¢çµæ§‹çš„åˆ†å‰²åŠŸèƒ½"""
    print("=== æ¸¬è©¦æ²’æœ‰é é¢çµæ§‹çš„æ–‡ä»¶åˆ†å‰² ===\n")
    
    # 1. å‰µå»ºæ¨¡æ“¬çš„ ConversionResult
    print("ğŸ”„ æ­¥é©Ÿ 1: å‰µå»ºæ¨¡æ“¬çš„ ConversionResultï¼ˆç„¡é é¢ä¿¡æ¯ï¼‰")
    conversion_result = create_mock_conversion_result_without_pages()
    
    print(f"âœ… æ¨¡æ“¬ ConversionResult å‰µå»ºå®Œæˆ")
    print(f"   - æª”å: {conversion_result.metadata.file_name}")
    print(f"   - æª”æ¡ˆé¡å‹: {conversion_result.metadata.file_type}")
    print(f"   - è½‰æ›å™¨: {conversion_result.metadata.converter_used}")
    print(f"   - ç¸½é æ•¸: {conversion_result.metadata.total_pages}")
    print(f"   - ç¸½è¡¨æ ¼æ•¸: {conversion_result.metadata.total_tables}")
    print(f"   - é é¢ä¿¡æ¯: {'æœ‰' if conversion_result.pages and len(conversion_result.pages) > 0 else 'ç„¡'}")
    print(f"   - å…§å®¹é•·åº¦: {len(conversion_result.content)}")
    
    # 2. åˆ†å‰²å…§å®¹
    print("\nâœ‚ï¸ æ­¥é©Ÿ 2: åˆ†å‰² Markdown å…§å®¹")
    
    # å‰µå»ºåˆ†å‰²å™¨
    splitter = ChunkSplitter(
        chunk_size=500,           # è¼ƒå°çš„ chunk å¤§å°
        chunk_overlap=100,        # è¼ƒå°çš„é‡ç–Š
        normalize_output=True,    # å•Ÿç”¨å…§å®¹æ­£è¦åŒ–
        keep_tables_together=True # ä¿æŒè¡¨æ ¼å®Œæ•´æ€§
    )
    
    # åˆ†å‰²å…§å®¹
    chunks = splitter.split_markdown(
        input_data=conversion_result,
        output_excel=True,         # è¼¸å‡º Excel æ–‡ä»¶
        output_path="service/output/chunk/no_pages_test.xlsx",
        md_output_path="service/output/md/no_pages_test.md"
    )
    
    print(f"âœ… åˆ†å‰²å®Œæˆ: å…± {len(chunks)} å€‹ chunks")
    
    # 3. åˆ†æçµæœ
    print("\nğŸ“Š æ­¥é©Ÿ 3: åˆ†æçµæœ")
    
    # åŸºæœ¬çµ±è¨ˆ
    total_length = sum(len(chunk.page_content) for chunk in chunks)
    avg_length = total_length / len(chunks) if chunks else 0
    
    # æª¢æŸ¥é ç¢¼æƒ…æ³
    page_numbers = [chunk.metadata.get('page_number') for chunk in chunks]
    has_page_numbers = any(pn is not None for pn in page_numbers)
    
    # è¡¨æ ¼ chunks
    table_chunks = [chunk for chunk in chunks if chunk.metadata.get('is_table', False)]
    
    print(f"   - ç¸½ chunks: {len(chunks)}")
    print(f"   - ç¸½å­—ç¬¦æ•¸: {total_length}")
    print(f"   - å¹³å‡é•·åº¦: {avg_length:.1f}")
    print(f"   - è¡¨æ ¼ chunks: {len(table_chunks)}")
    print(f"   - ä¸€èˆ¬ chunks: {len(chunks) - len(table_chunks)}")
    print(f"   - é ç¢¼æƒ…æ³: {'æœ‰é ç¢¼' if has_page_numbers else 'ç„¡é ç¢¼ï¼ˆç¬¦åˆé æœŸï¼‰'}")
    
    # 4. æª¢æŸ¥ metadata
    print("\nğŸ“ æ­¥é©Ÿ 4: æª¢æŸ¥ Metadata")
    if chunks:
        sample_chunk = chunks[0]
        print(f"ç¯„ä¾‹ chunk metadata:")
        for key, value in sample_chunk.metadata.items():
            if key not in ['Header 1', 'Header 2', 'Header 3', 'Header 4']:
                print(f"   - {key}: {value}")
    
    # 5. é¡¯ç¤ºæ‰€æœ‰ chunks
    print("\nğŸ“„ æ­¥é©Ÿ 5: æ‰€æœ‰ Chunks é è¦½")
    for i, chunk in enumerate(chunks, 1):
        print(f"\n--- Chunk {i} ---")
        print(f"é•·åº¦: {len(chunk.page_content)}")
        print(f"é ç¢¼: {chunk.metadata.get('page_number', 'N/A')}")
        print(f"æ¨™é¡Œç´šæ•¸: {chunk.metadata.get('Header 1', 'N/A')}")
        print(f"å…§å®¹é è¦½: {chunk.page_content[:150]}...")
    
    print(f"\nğŸ‰ æ²’æœ‰é é¢çµæ§‹çš„æ–‡ä»¶åˆ†å‰²æ¸¬è©¦å®Œæˆï¼")
    print(f"ğŸ“ Excel æ–‡ä»¶: service/output/chunk/no_pages_test.xlsx")
    print(f"ğŸ“ Markdown æ–‡ä»¶: service/output/md/no_pages_test.md")
    
    return chunks

def test_metadata_consistency_no_pages(chunks: List[Document]):
    """æ¸¬è©¦æ²’æœ‰é é¢çµæ§‹çš„ metadata ä¸€è‡´æ€§"""
    print("\n=== æ¸¬è©¦ Metadata ä¸€è‡´æ€§ï¼ˆç„¡é é¢çµæ§‹ï¼‰ ===\n")
    
    # æª¢æŸ¥æ‰€æœ‰ chunks çš„ metadata ä¸€è‡´æ€§
    required_fields = ['file_name', 'file_type', 'source', 'converter_used', 'total_pages', 'total_tables']
    
    print("æª¢æŸ¥å¿…è¦ metadata æ¬„ä½:")
    for field in required_fields:
        missing_count = sum(1 for chunk in chunks if field not in chunk.metadata)
        print(f"   - {field}: {len(chunks) - missing_count}/{len(chunks)} chunks åŒ…å«æ­¤æ¬„ä½")
    
    # æª¢æŸ¥é ç¢¼æƒ…æ³
    page_number_count = sum(1 for chunk in chunks if chunk.metadata.get('page_number') is not None)
    print(f"   - page_number: {page_number_count}/{len(chunks)} chunks æœ‰é ç¢¼ï¼ˆé æœŸç‚º 0ï¼‰")
    
    # æª¢æŸ¥æ˜¯å¦æœ‰å…¶ä»–ä¸æ‡‰è©²å­˜åœ¨çš„é é¢ç›¸é—œä¿¡æ¯
    page_related_fields = ['page_number']
    for field in page_related_fields:
        present_count = sum(1 for chunk in chunks if chunk.metadata.get(field) is not None)
        print(f"   - {field}: {present_count}/{len(chunks)} chunks æœ‰æ­¤æ¬„ä½ï¼ˆé æœŸç‚º 0ï¼‰")
    
    print("âœ… Metadata ä¸€è‡´æ€§æª¢æŸ¥å®Œæˆ")

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("=== æ²’æœ‰é é¢çµæ§‹çš„æ–‡ä»¶æ”¯æ´åŠŸèƒ½æ¸¬è©¦ ===\n")
    
    try:
        # æ¸¬è©¦æ²’æœ‰é é¢çµæ§‹çš„åˆ†å‰²åŠŸèƒ½
        chunks = test_no_pages_splitting()
        
        if chunks:
            # æ¸¬è©¦ metadata ä¸€è‡´æ€§
            test_metadata_consistency_no_pages(chunks)
            
            print(f"\nğŸ‰ æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼")
        else:
            print(f"\nâŒ æ¸¬è©¦å¤±æ•—")
            
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
